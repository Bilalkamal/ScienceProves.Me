{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download arXiv dataset using Kaggle API\n",
    "\n",
    "# Install required package\n",
    "%pip install kaggle\n",
    "\n",
    "# Set up Kaggle credentials and permissions\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download and extract dataset\n",
    "!kaggle datasets download -d Cornell-University/arxiv\n",
    "!unzip arxiv.zip\n",
    "\n",
    "# Verify file exists\n",
    "import os\n",
    "\n",
    "filename = \"arxiv-metadata-oai-snapshot.json\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"Successfully downloaded {filename}\")\n",
    "    !ls -lh {filename}\n",
    "else:\n",
    "    print(f\"Error: {filename} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas requests backoff python-dotenv openai supabase tqdm pydantic aiohttp nest_asyncio habanero pydoi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from supabase import create_client\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------- LOGGING ------------------- #\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # Set to WARNING to suppress INFO logs\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ------------------- ENV & GLOBALS ------------------- #\n",
    "# load_dotenv()  # Load .env if present\n",
    "\n",
    "# It's recommended to use environment variables for sensitive information\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_SERVICE_ROLE_KEY = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)\n",
    "\n",
    "# How many lines per chunk in the JSONL file\n",
    "CHUNK_SIZE = 500\n",
    "# We'll wait 1s between each chunk\n",
    "SLEEP_BETWEEN_CHUNKS = 1\n",
    "# Progress file to track the last completed chunk index (optional for multi-machine scenario)\n",
    "PROGRESS_FILE = \"progress.txt\"\n",
    "\n",
    "# Default short timeout for each fallback method\n",
    "FALLBACK_TIMEOUT = 1.0\n",
    "\n",
    "# ------------------- JOURNAL FALLBACK METHODS (SHORT TIMEOUTS) ------------------- #\n",
    "try:\n",
    "    from habanero import Crossref\n",
    "except ImportError:\n",
    "    logger.warning(\"habanero not installed. `pip install habanero` if needed.\")\n",
    "    Crossref = None\n",
    "\n",
    "\n",
    "from pydoi import resolve as pydoi_resolve\n",
    "\n",
    "\n",
    "\n",
    "def get_journal_via_crossref(doi: str, timeout: float = 3.0) -> Optional[str]:\n",
    "    \"\"\"Attempt CrossRef (requests) call, with a short timeout.\"\"\"\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    resp = requests.get(url, timeout=timeout)\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        jrnl = data[\"message\"].get(\"container-title\", [])\n",
    "        if jrnl:\n",
    "            return jrnl[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_journal_via_habanero(doi: str, timeout: float = 3.0) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Using habanero library, within a short overall timeframe.\n",
    "    We'll do a direct requests fallback if library is missing or fails.\n",
    "    \"\"\"\n",
    "    if Crossref is None:\n",
    "        return None\n",
    "    try:\n",
    "        cr = Crossref()\n",
    "        result = cr.works(ids=doi)\n",
    "        jrnl = result[\"message\"].get(\"container-title\", [])\n",
    "        if jrnl:\n",
    "            return jrnl[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_journal_via_pydoi(doi: str, timeout: float = 3.0) -> Optional[str]:\n",
    "    \"\"\"Using pydoi library, with short fallback.\"\"\"\n",
    "    if pydoi_resolve is None:\n",
    "        return None\n",
    "    try:\n",
    "        metadata = pydoi_resolve(doi)\n",
    "        ctitle = metadata.get(\"container-title\")\n",
    "        if isinstance(ctitle, list) and ctitle:\n",
    "            return ctitle[0]\n",
    "        elif isinstance(ctitle, str):\n",
    "            return ctitle\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_journal_via_doi2bib(doi: str, timeout: float = 3.0) -> Optional[str]:\n",
    "    \"\"\"Scrape from doi2bib.org.\"\"\"\n",
    "    url = f\"https://doi2bib.org/bib/{doi}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        if response.status_code == 200:\n",
    "            bibtex = response.text\n",
    "            match = re.search(r'journal\\s*=\\s*{([^}]+)}', bibtex, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "emails = [\n",
    "   \"Add emails here\"\n",
    "]\n",
    "email_cycle = itertools.cycle(emails)\n",
    "\n",
    "\n",
    "def get_journal_via_unpaywall(doi: str, timeout: float = 3.0) -> Optional[str]:\n",
    "    \"\"\"Using Unpaywall with rotating emails, short timeout.\"\"\"\n",
    "    email = next(email_cycle)\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\"\n",
    "    params = {\"email\": email}\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            return data.get(\"journal_name\")  # or None\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "ALL_JOURNAL_METHODS = [\n",
    "    get_journal_via_crossref,\n",
    "    get_journal_via_habanero,\n",
    "    get_journal_via_pydoi,\n",
    "    get_journal_via_doi2bib,\n",
    "    get_journal_via_unpaywall,\n",
    "]\n",
    "\n",
    "\n",
    "def get_journal_multi(doi: str, fallback_timeout: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Try each fallback method in **random order** so we don't always\n",
    "    wait for a slow method first. Each has a short timeout (~1s).\n",
    "    If all fail or return nothing, raise ValueError.\n",
    "    \"\"\"\n",
    "    normalized = doi.replace(\"https://doi.org/\", \"\").strip()\n",
    "    if not normalized:\n",
    "        raise ValueError(\"DOI is empty or invalid.\")\n",
    "\n",
    "    methods = ALL_JOURNAL_METHODS[:]\n",
    "    random.shuffle(methods)  # randomize fallback order\n",
    "\n",
    "    for fn in methods:\n",
    "        try:\n",
    "            jrnl = fn(normalized, timeout=fallback_timeout)\n",
    "            if jrnl and jrnl.lower() not in (\"unknown journal\", \"journal not found\"):\n",
    "                return jrnl\n",
    "        except Exception:\n",
    "            # We ignore the error and move on\n",
    "            pass\n",
    "\n",
    "    raise ValueError(\"All fallback methods failed or timed out.\")\n",
    "\n",
    "\n",
    "# ------------------- Pydantic Models ------------------- #\n",
    "class Metadata(BaseModel):\n",
    "    date: Optional[str] = None\n",
    "    journal_ref: Optional[str] = None\n",
    "    journal_title: Optional[str] = None\n",
    "    source: str = 'arxiv'\n",
    "    authors: Optional[List[List[str]]] = None\n",
    "    categories: Optional[List[str]] = None\n",
    "\n",
    "\n",
    "# ------------------- EMBEDDINGS MANAGER ------------------- #\n",
    "class EmbeddingsManager:\n",
    "    def __init__(self, openai_api_key: Optional[str] = None, default_model: str = \"text-embedding-3-small\"):\n",
    "        if not openai_api_key:\n",
    "            openai_api_key = \"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\"\n",
    "        openai.api_key = openai_api_key\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.default_model = default_model\n",
    "\n",
    "    def get_embeddings(self, text: str, model: Optional[str] = None) -> List[float]:\n",
    "        if model is None:\n",
    "            model = self.default_model\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        response = self.client.embeddings.create(model=model, input=text)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "\n",
    "embeddings_manager = EmbeddingsManager()\n",
    "\n",
    "\n",
    "def _get_embedding_for_text(text: str) -> List[float]:\n",
    "    return embeddings_manager.get_embeddings(text)\n",
    "\n",
    "\n",
    "# ------------------- SUPABASE INSERT ------------------- #\n",
    "def batch_insert_documents(df: pd.DataFrame, batch_size: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Insert the valid records in sub-batches of size batch_size.\n",
    "    \"\"\"\n",
    "    total_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i : i + batch_size]\n",
    "        batch_data = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            batch_data.append({\n",
    "                \"title\": row.get(\"title\", \"Untitled\"),\n",
    "                \"content\": row.get(\"content\", \"\"),\n",
    "                \"embedding\": row.get(\"embedding\", []),\n",
    "                \"url\": row.get(\"url\", None),\n",
    "                \"metadata\": row.get(\"metadata\", {}) if isinstance(row.get(\"metadata\"), dict) else {}\n",
    "            })\n",
    "        try:\n",
    "            response = supabase.table(\"documents\").insert(batch_data).execute()\n",
    "            if not response.data:\n",
    "                logger.warning(f\"No data returned for sub-batch {i//batch_size + 1}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error inserting sub-batch {i//batch_size + 1}: {str(e)}\")\n",
    "\n",
    "\n",
    "# ------------------- PROGRESS HELPER ------------------- #\n",
    "def load_progress() -> int:\n",
    "    \"\"\"\n",
    "    Reads the last completed chunk index from PROGRESS_FILE.\n",
    "    Returns 0 if file not found or empty.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(PROGRESS_FILE):\n",
    "        return 0\n",
    "    try:\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            line = f.read().strip()\n",
    "            return int(line)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def save_progress(chunk_index: int):\n",
    "    \"\"\"\n",
    "    Saves the last completed chunk index to PROGRESS_FILE.\n",
    "    Overwrites previous content.\n",
    "    \"\"\"\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        f.write(str(chunk_index))\n",
    "\n",
    "\n",
    "# ------------------- ASYNC WRAPPER FOR JOURNAL LOOKUPS ------------------- #\n",
    "@backoff.on_exception(backoff.expo, (aiohttp.ClientError, asyncio.TimeoutError), max_tries=3)\n",
    "async def _fetch_journal(session: aiohttp.ClientSession, doi: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs get_journal_multi() in a thread executor (to avoid blocking the event loop).\n",
    "    If it fails => raise exception => we'll store \"Error: ...\".\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, partial(get_journal_multi, doi, FALLBACK_TIMEOUT))\n",
    "\n",
    "\n",
    "async def fetch_journals_in_bulk(dois: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Concurrency for each chunk of DOIs.\n",
    "    Returns { original_doi: journal_or_error_msg }.\n",
    "    If an error occurs or all methods fail, we store \"Error: ...\" so we can skip it.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for doi in dois:\n",
    "            tasks.append(asyncio.ensure_future(_fetch_journal(session, doi)))\n",
    "\n",
    "        fetch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    # Count successes / errors\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    for doi, result in zip(dois, fetch_results):\n",
    "        if isinstance(result, Exception):\n",
    "            results[doi] = f\"Error: {result}\"\n",
    "            error_count += 1\n",
    "        else:\n",
    "            results[doi] = result\n",
    "            success_count += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------- READ JSONL IN CHUNKS ------------------- #\n",
    "def read_jsonl_in_chunks(file_path: Path | str, chunk_size: int = 100):\n",
    "    \"\"\"\n",
    "    Generator that yields chunks of data (list of dicts),\n",
    "    each chunk up to 'chunk_size' lines from the JSONL file.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            batch.append(record)\n",
    "            if len(batch) >= chunk_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# ------------------- MAIN PIPELINE ------------------- #\n",
    "def main(\n",
    "    file_path: str = \"arxiv-metadata-oai-snapshot.json\",\n",
    "    start_chunk: int = 1,\n",
    "    end_chunk: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads lines in chunks. For each chunk:\n",
    "      - Only processes if chunk_counter >= start_chunk\n",
    "      - If end_chunk is provided, stop if chunk_counter > end_chunk\n",
    "      - Fetch journal info concurrently\n",
    "      - Filter out records with invalid/no journal\n",
    "      - Generate embeddings concurrently\n",
    "      - Batch-insert to Supabase\n",
    "      - Wait 1 second between chunks\n",
    "\n",
    "    If you want to fully ignore the progress file logic, just remove or comment it out.\n",
    "    If you still want the script to skip already processed chunks on each machine,\n",
    "    you could mix `load_progress()` logic with the [start_chunk, end_chunk] range.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load progress\n",
    "    current_progress = load_progress()\n",
    "\n",
    "    chunk_counter = 0\n",
    "    for batch_data in read_jsonl_in_chunks(file_path, chunk_size=CHUNK_SIZE):\n",
    "        chunk_counter += 1\n",
    "\n",
    "        # Skip already processed chunks\n",
    "        if chunk_counter <= current_progress:\n",
    "            continue\n",
    "\n",
    "        # Skip if below start_chunk\n",
    "        if chunk_counter < start_chunk:\n",
    "            continue\n",
    "        # Break if we've passed end_chunk (when end_chunk is not None)\n",
    "        if end_chunk is not None and chunk_counter > end_chunk:\n",
    "            break\n",
    "\n",
    "        # Print current chunk being processed\n",
    "        print(f\"Processing chunk #{chunk_counter}\")\n",
    "\n",
    "        chunk_start = time.time()\n",
    "\n",
    "        # ---------------- Step 1: Convert to DF & Basic Cleanup ---------------- #\n",
    "        print(\"Processing Step: Convert + Cleanup\")\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        if \"doi\" not in df.columns:\n",
    "            df[\"doi\"] = \"\"\n",
    "\n",
    "        df[\"doi\"] = df[\"doi\"].fillna(\"\").astype(str).str.strip()\n",
    "        valid_dois = df.loc[df[\"doi\"] != \"\", \"doi\"].tolist()\n",
    "\n",
    "        # ---------------- Step 2: Fetch Journals Concurrently ---------------- #\n",
    "        doi_to_journal = {}\n",
    "        if valid_dois:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            doi_to_journal = loop.run_until_complete(fetch_journals_in_bulk(valid_dois))\n",
    "\n",
    "        # ---------------- Step 3: Filter + Build Processed Data ---------------- #\n",
    "        print(\"Processing Step: Filter + Build Processed Data\")\n",
    "        processed_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            doi_str = row[\"doi\"]\n",
    "            if not doi_str:\n",
    "                continue  # skip if no doi\n",
    "\n",
    "            jrnl = doi_to_journal.get(doi_str, None)\n",
    "            if not jrnl or jrnl.startswith(\"Error:\"):\n",
    "                continue\n",
    "\n",
    "            jrnl_clean = jrnl.strip()\n",
    "            if not jrnl_clean or jrnl_clean.lower() in (\"unknown journal\", \"journal not found\"):\n",
    "                continue\n",
    "\n",
    "            url_str = doi_str if doi_str.startswith(\"https://doi.org/\") else f\"https://doi.org/{doi_str}\"\n",
    "            processed_data.append({\n",
    "                \"title\": row.get(\"title\", \"\"),\n",
    "                \"content\": row.get(\"abstract\", \"\"),\n",
    "                \"url\": url_str,\n",
    "                \"metadata\": Metadata(\n",
    "                    date=row.get(\"update_date\"),\n",
    "                    journal_ref=row.get(\"journal-ref\"),\n",
    "                    journal_title=jrnl_clean,\n",
    "                    source=\"arxiv\",\n",
    "                    authors=row.get(\"authors_parsed\"),\n",
    "                    categories=row[\"categories\"].split() if row.get(\"categories\") else None\n",
    "                ).model_dump(),\n",
    "            })\n",
    "\n",
    "        successful_docs = len(processed_data)\n",
    "        print(f\"Number of successful docs processed: {successful_docs}\")\n",
    "\n",
    "        if not processed_data:\n",
    "            # Update progress and continue to next chunk\n",
    "            save_progress(chunk_counter)\n",
    "            print(\"-----\")\n",
    "            time.sleep(SLEEP_BETWEEN_CHUNKS)\n",
    "            continue\n",
    "\n",
    "        processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "        # ---------------- Step 4: Generate Embeddings Concurrency ---------------- #\n",
    "        print(\"Processing Step: Generate Embeddings\")\n",
    "        contents = processed_df[\"content\"].fillna(\"\").tolist()\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            embeddings_list = list(\n",
    "                tqdm(\n",
    "                    executor.map(_get_embedding_for_text, contents),\n",
    "                    total=len(contents),\n",
    "                    desc=f\"Embedding chunk {chunk_counter}\",\n",
    "                    disable=True  # Disable tqdm progress bar\n",
    "                )\n",
    "            )\n",
    "        processed_df[\"embedding\"] = embeddings_list\n",
    "\n",
    "        # ---------------- Step 5: Insert into Supabase ---------------- #\n",
    "        batch_insert_documents(processed_df, batch_size=100)\n",
    "\n",
    "        # Update progress after successful processing\n",
    "        save_progress(chunk_counter)\n",
    "\n",
    "        chunk_end = time.time()\n",
    "        elapsed = chunk_end - chunk_start\n",
    "\n",
    "        # Add separator after processing the chunk\n",
    "        print(\"-----\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN_CHUNKS)\n",
    "\n",
    "    # Final message after all chunks are processed\n",
    "    print(f\"All done! Processed up to chunk #{chunk_counter}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = 1\n",
    "    end = 5000\n",
    "    main(\"arxiv-metadata-oai-snapshot.json\", start_chunk=start, end_chunk=end)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
